[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1 Welcome to my practicals website!!",
    "section": "",
    "text": "1 Welcome to my practicals website!!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to my practicals website!!</span>"
    ]
  },
  {
    "objectID": "index.html#feel-at-home",
    "href": "index.html#feel-at-home",
    "title": "1 Welcome to my practicals website!!",
    "section": "1.1 Feel at home",
    "text": "1.1 Feel at home\nThis is the introduction page of my site for the practicals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to my practicals website!!</span>"
    ]
  },
  {
    "objectID": "PRAC_1.html",
    "href": "PRAC_1.html",
    "title": "2  Question 1",
    "section": "",
    "text": "Airquality (available in R) was utilised for this question. Here is a sneak peak of the dataset:\n\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nThe data consists of Ozone, Solar.R, Wind, Temp, Month, and Day columns.\n\n\nCode\nquality_data = airquality\nhead(quality_data[(is.na(quality_data$Ozone) | is.na(quality_data$Temp)),])\n\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n10    NA     194  8.6   69     5  10\n25    NA      66 16.6   57     5  25\n26    NA     266 14.9   58     5  26\n27    NA      NA  8.0   57     5  27\n32    NA     286  8.6   78     6   1\n\n\nCode\ntail(quality_data[(is.na(quality_data$Ozone) | is.na(quality_data$Temp)),])\n\n\n    Ozone Solar.R Wind Temp Month Day\n102    NA     222  8.6   92     8  10\n103    NA     137 11.5   86     8  11\n107    NA      64 11.5   79     8  15\n115    NA     255 12.6   75     8  23\n119    NA     153  5.7   88     8  27\n150    NA     145 13.2   77     9  27\n\n\n\n3 Question 2\nThe following statistics: mean, sd, min, and max were derived for each of temperature and ozone level, and they are as follows:\n\n\nCode\ntemp_info = data.frame(\"Mean\" = mean(quality_data$Temp),\n                       \"Sd\" = sd(quality_data$Temp), \n                       \"Min\" = min(quality_data$Temp), \n                       \"Max\" = max(quality_data$Temp))\ntemp_info\n\n\n      Mean      Sd Min Max\n1 77.88235 9.46527  56  97\n\n\nHowever, the Ozone column has a few rows with missing data(NAs):\n\n\n[OUTPUT] Number of missing values for Ozone column:      37\n\n\nSo a slight adjustment was made to the code to find the missing values. Please select the Code button below to see the slight adjustment. However, here are the statistics:\n\n\nCode\nOzone_info = data.frame(\"Mean\" = mean(quality_data$Ozone, na.rm = T),\n                       \"Sd\" = sd(quality_data$Ozone, na.rm = T), \n                       \"Min\" = min(quality_data$Ozone, na.rm = T), \n                       \"Max\" = max(quality_data$Ozone, na.rm = T))\n\nOzone_info\n\n\n      Mean       Sd Min Max\n1 42.12931 32.98788   1 168\n\n\n\n\n4 Question 3\nNow we move to a different dataset called Cars available in R. Here is a sneak peak of the data:\n\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n\nIn this section we wanted to, from first principle or Using matrix calculations, find linear regression beta estimates, t-statistics, r squared, standard errors, etc, and fortunately we found the following:\n\n\n$Betas_estimates\n           [,1]\n[1,] -17.579095\n[2,]   3.932409\n\n$RSS\n[1] 11353.52\n\n$R_squared\n[1] 0.6510794\n\n$Standard_errors\n[1] 6.7584402 0.4155128\n\n$P_values\n             [,1]\n[1,] 1.231882e-02\n[2,] 1.489919e-12\n\n$T_statistics\n          [,1]\n[1,] -2.601058\n[2,]  9.463990\n\n\n\n\n5 Question 4\nNow we use lm() function in R to fit the same data and evaluate whether the estimates from the model are consistent with our first principles or not:\n\n\n\nCall:\nlm(formula = dist ~ speed, data = cars_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nAs you can see, the results are quite the same. Thank God!! We are so smart! Interpretation: As speed increases by one unit, distance to stop increases by 3.932 (holding everything else constant).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question 1</span>"
    ]
  },
  {
    "objectID": "PRAC_2.html",
    "href": "PRAC_2.html",
    "title": "3  Lowess Smoothing",
    "section": "",
    "text": "In this practical our aim is to create a custom Lowess function that smoothes points out from a given data.\n\n3.0.1 Data Simulation\nWe start first by simulating our x and y values.\n\n\nCode\nset.seed(1)\nx = seq(1,100)\ny = numeric(length(x))\ne = rnorm(100,0,0.2^2)\n\nfor (i in 1:100){\n  y[i] = sin(x[i]/10) + e[i]\n}\nplot(x,y)\n\n\n\n\n\n\n\n\n\n\n\n3.0.2 Lowess Function Code\nNow we code the custom Lowess function.\n\ncustomLowess = function(x, y, f){\n  n = length(x)  # n = Number of observations\n  w = numeric(n)      \n  y_hat = numeric(n)     \n  k = ceiling(f * n) # k = Number of nearest neighbours\n  \n  for (i in 1:n) {  \n    dists = abs(x - x[i])   # Calculating the distances\n    d_max = sort(dists)[k]   # Finding the max distance\n    w = (1 - (dists / d_max)^3)^3 # Calculating the weights\n    w[w &lt; 0] = 0        \n    X = cbind(1, x)\n    W = diag(w) \n    B_hat = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y   # Calculating the betas\n    y_hat[i] = B_hat[1] + B_hat[2] * x[i]\n  }\n  \n  return(y_hat)  # Returning a complete smoothed out vector of y values.\n}\n\n\n\n3.0.3 Plots\nFollowing from the code, the next step is to evaluate the results and see whether the smoothed plot from the R Lowess function is the same as the plot of of the smoothed values from our custom. For purposes of demonstration, we have chosen the value of 0.3 for our span along with the simulated points for our x and y values. Here are the plots:\n\n\nCode\nsmoothed_y = customLowess(x,y, 0.3)\nlowess_fit = lowess(x, y, f = 0.3, iter = 0)\nplot(x,y,ylab = \"Smoothed y values\", xlab = \"x values\", \n     main = \"Custom Lowess function\")\nlines(x,smoothed_y, col = \"blue\")\n\n\n\n\n\n\n\n\n\nCode\nplot(x,y,ylab = \"Smoothed y values\", xlab = \"x values\", \n     main = \"R Lowess function\")\nlines(x, lowess_fit$y, col = \"red\" )\n\n\n\n\n\n\n\n\n\nIndeed, the results are identical. Our custom function gives us correct output.\n\n3.0.3.1 Additional evaluation\nAside from the plots, we can also evaluate the actual y-values we get from both functions. Let us see if they are identical or not:\n\n\nThe following are the y-values from the R lowess function \n\n\n  [1]  0.34710017  0.39047183  0.43263675  0.47357343  0.51329887  0.55184898\n  [7]  0.58927625  0.62565069  0.66104963  0.69555158  0.72923752  0.76219405\n [13]  0.79446320  0.82573435  0.85390345  0.85477534  0.84694558  0.83072359\n [19]  0.80644090  0.77438053  0.73487249  0.68825329  0.63484505  0.57501564\n [25]  0.50926755  0.43828178  0.36276309  0.28349765  0.20141028  0.11754415\n [31]  0.03283910 -0.05184156 -0.13571005 -0.21800769 -0.29798460 -0.37493872\n [37] -0.44816878 -0.51699017 -0.58072250 -0.63867957 -0.69035207 -0.73515941\n [43] -0.77246584 -0.80178674 -0.82290182 -0.83574904 -0.84033028 -0.83655113\n [49] -0.82442384 -0.80401281 -0.77544728 -0.73902584 -0.69516286 -0.64441418\n [55] -0.58732249 -0.52439297 -0.45608405 -0.38302123 -0.30601036 -0.22585327\n [61] -0.14337950 -0.05947667  0.02493150  0.10892358  0.19163880  0.27224098\n [67]  0.34995514  0.42409336  0.49405614  0.55929630  0.61909355  0.67278976\n [73]  0.71994319  0.76022308  0.79320752  0.81848288  0.83563495  0.84439306\n [79]  0.84468828  0.83664095  0.82050673  0.79663793  0.76538229  0.72686732\n [85]  0.68119246  0.62849287  0.55577528  0.48141351  0.40653252  0.33119784\n [91]  0.25532439  0.17884076  0.10169934  0.02386028 -0.05471195 -0.13404715\n [97] -0.21417084 -0.29509538 -0.37680843 -0.45927386\n\n\nThe following are the y-values from the custom function \n\n\n  [1]  0.34710017  0.39047183  0.43263675  0.47357343  0.51329887  0.55184898\n  [7]  0.58927625  0.62565069  0.66104963  0.69555158  0.72923752  0.76219405\n [13]  0.79446320  0.82573435  0.85390345  0.85477534  0.84694558  0.83072359\n [19]  0.80644090  0.77438053  0.73487249  0.68825329  0.63484505  0.57501564\n [25]  0.50926755  0.43828178  0.36276309  0.28349765  0.20141028  0.11754415\n [31]  0.03283910 -0.05184156 -0.13571005 -0.21800769 -0.29798460 -0.37493872\n [37] -0.44816878 -0.51699017 -0.58072250 -0.63867957 -0.69035207 -0.73515941\n [43] -0.77246584 -0.80178674 -0.82290182 -0.83574904 -0.84033028 -0.83655113\n [49] -0.82442384 -0.80401281 -0.77544728 -0.73902584 -0.69516286 -0.64441418\n [55] -0.58732249 -0.52439297 -0.45608405 -0.38302123 -0.30601036 -0.22585327\n [61] -0.14337950 -0.05947667  0.02493150  0.10892358  0.19163880  0.27224098\n [67]  0.34995514  0.42409336  0.49405614  0.55929630  0.61909355  0.67278976\n [73]  0.71994319  0.76022308  0.79320752  0.81848288  0.83563495  0.84439306\n [79]  0.84468828  0.83664095  0.82050673  0.79663793  0.76538229  0.72686732\n [85]  0.68119246  0.62849287  0.55577528  0.48141351  0.40653252  0.33119784\n [91]  0.25532439  0.17884076  0.10169934  0.02386028 -0.05471195 -0.13404715\n [97] -0.21417084 -0.29509538 -0.37680843 -0.45927386\n\n\nThe results are indeed the same again.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lowess Smoothing</span>"
    ]
  },
  {
    "objectID": "PRAC_DAY_4.html",
    "href": "PRAC_DAY_4.html",
    "title": "4  Question 1",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n5 Question 2\nUnder this code, is the same code rewritten using dplyr and pipe:\n\nflight1 &lt;- flights[flights$month == 1, ]\ncarrier_vec &lt;- unique(flight1$carrier)\ncarrier_dist_vec_mean &lt;- numeric(length(carrier_vec))\ncarrier_dist_vec_sd &lt;- numeric(length(carrier_vec))\nfor (i in seq_along(carrier_vec)) {\n  carrier_dist_vec_mean[i] &lt;- mean(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n   )\n  carrier_dist_vec_sd[i] &lt;- sd(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n  )\n}\ndist_tbl &lt;- tibble(\n  carrier = carrier_vec,\n  mean_distance = carrier_dist_vec_mean,\n  sd_distance = carrier_dist_vec_sd\n)\ndist_tbl[order(dist_tbl$mean_distance), ]\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0  \n\n\n\nflights1 = flights |&gt; filter(month == 1)\ncarrier_vec = flights |&gt; distinct(carrier) |&gt; pull()\ncarrier_dist_vec_mean &lt;- carrier_vec |&gt; length() |&gt; numeric()\ncarrier_dist_vec_sd &lt;- carrier_vec |&gt; length() |&gt; numeric()   \n\n\nfor (i in carrier_vec |&gt; seq_along()){\n  carrier_dist_vec_mean[i] = flights1 |&gt;\n    filter(carrier == carrier_vec[i]) |&gt; \n    pull(distance) |&gt; mean()\n  carrier_dist_vec_sd[i] = flights |&gt; filter(carrier == carrier_vec[i]) |&gt;\n    pull(distance) |&gt; sd()\n}\n\ndist_tbl &lt;- tibble(\n  carrier = carrier_vec,\n  mean_distance = carrier_dist_vec_mean,\n  sd_distance = carrier_dist_vec_sd\n)\n\ndist_tbl |&gt; arrange(mean_distance)\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229        160. \n 2 9E               476.       322. \n 3 EV               522.       287. \n 4 US               536.       584. \n 5 MQ               566.       226. \n 6 FL               691.       161. \n 7 OO               733        206. \n 8 WN               942.       410. \n 9 B6              1062.       704. \n10 DL              1220.       660. \n11 AA              1350.       638. \n12 UA              1462.       799. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        88.0\n16 HA              4983          0  \n\n\n\n\n6 Question 3\n\n\n7 Question 4\n\nflights |&gt; group_by(month, carrier) |&gt; \n  summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  pivot_wider(\n              names_from = \"carrier\",\n              values_from = \"avg_dep_delay\"\n              )\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 17\n# Groups:   month [12]\n   month  `9E`    AA     AS    B6    DL    EV    F9    FL    HA    MQ    OO\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 16.9   6.93  7.35   9.49  3.85 24.2  10     1.97 54.4   6.49 67   \n 2     2 16.5   8.28  0.722 13.8   5.54 21.5  29.8   5.18 17.4   8.09 NA   \n 3     3 13.4   8.70  8.42  14.2   9.93 26.2  16.8  17.3   1.16  7.19 NA   \n 4     4 13.6  11.7  11.3   15.2   8.17 22.8  24.6  13.1  -2.1  13.7  NA   \n 5     5 22.7   9.66  6.77   9.78  9.74 20.2  35.9  19.2  -1.45 13.9  NA   \n 6     6 29.0  14.6  13.1   20.4  18.7  25.5  29.4  38.8   1.47 20.8  61   \n 7     7 31.4  12.1   2.42  24.9  20.6  26.5  31.8  41.2  -1.71 20.7  NA   \n 8     8 17.3   7.17  2.87  15.7   9.85 16.3  22.2  23.4   1.68 10.1  64   \n 9     9  7.75  5.69 -4.52   6.63  5.53  8.24  8.26 16.9  -5.44  5.35 -4.94\n10    10  9.33  3.00  0.677  2.96  3.42 13.4   9.70 13.7  -5.10  4.48 NA   \n11    11  7.56  3.10  3.08   3.52  2.85  9.83 13.5  16.9  -5.44  3.28  0.8 \n12    12 19.8  11.7  18.0   17.0  10.8  27.9  13.1  26.1  -3.14 12.7  NA   \n# ℹ 5 more variables: UA &lt;dbl&gt;, US &lt;dbl&gt;, VX &lt;dbl&gt;, WN &lt;dbl&gt;, YV &lt;dbl&gt;\n\n\n\n\n8 Question 5\n\nflights |&gt; filter((dep_delay &gt; 0 & arr_delay &lt;= 0)) |&gt; \n  summarise(proportion = n()/ nrow(flights))\n\n# A tibble: 1 × 1\n  proportion\n       &lt;dbl&gt;\n1      0.105\n\n\n\n\n9 Question 6\n\na.\n\nflights |&gt; group_by(origin,dest) |&gt;\n  summarise(more_than_one = n_distinct(carrier)) |&gt;\n  filter(more_than_one &gt; 1) \n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 128 × 3\n# Groups:   origin [3]\n   origin dest  more_than_one\n   &lt;chr&gt;  &lt;chr&gt;         &lt;int&gt;\n 1 EWR    ATL               4\n 2 EWR    AUS               2\n 3 EWR    BDL               2\n 4 EWR    BNA               2\n 5 EWR    BOS               3\n 6 EWR    BWI               2\n 7 EWR    CHS               2\n 8 EWR    CLE               2\n 9 EWR    CLT               3\n10 EWR    CVG               2\n# ℹ 118 more rows\n\n\n\n\nb.\n\nflights |&gt; group_by(origin,dest, carrier) |&gt;\n  summarise(avg_arr_delay = mean(arr_delay,na.rm = T)) |&gt;\n  select(origin, dest, avg_arr_delay, carrier)\n\n`summarise()` has grouped output by 'origin', 'dest'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 439 × 4\n# Groups:   origin, dest [224]\n   origin dest  avg_arr_delay carrier\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1 EWR    ALB           14.4  EV     \n 2 EWR    ANC           -2.5  UA     \n 3 EWR    ATL           -6.25 9E     \n 4 EWR    ATL           10.0  DL     \n 5 EWR    ATL           19.5  EV     \n 6 EWR    ATL           10.5  UA     \n 7 EWR    AUS            4.28 UA     \n 8 EWR    AUS          -11.2  WN     \n 9 EWR    AVL            8.80 EV     \n10 EWR    BDL            6.78 EV     \n# ℹ 429 more rows\n\n\n\n\nc.\n\nflights_c = flights |&gt; group_by(origin,dest, carrier) |&gt;\n  summarise(avg_arr_delay = mean(arr_delay,na.rm = T)) |&gt;\n  left_join(airlines, by = \"carrier\") |&gt;\n  select(origin, dest, carrier, name, avg_arr_delay)\n\n`summarise()` has grouped output by 'origin', 'dest'. You can override using\nthe `.groups` argument.\n\nflights_c\n\n# A tibble: 439 × 5\n# Groups:   origin, dest [224]\n   origin dest  carrier name                     avg_arr_delay\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                            &lt;dbl&gt;\n 1 EWR    ALB   EV      ExpressJet Airlines Inc.         14.4 \n 2 EWR    ANC   UA      United Air Lines Inc.            -2.5 \n 3 EWR    ATL   9E      Endeavor Air Inc.                -6.25\n 4 EWR    ATL   DL      Delta Air Lines Inc.             10.0 \n 5 EWR    ATL   EV      ExpressJet Airlines Inc.         19.5 \n 6 EWR    ATL   UA      United Air Lines Inc.            10.5 \n 7 EWR    AUS   UA      United Air Lines Inc.             4.28\n 8 EWR    AUS   WN      Southwest Airlines Co.          -11.2 \n 9 EWR    AVL   EV      ExpressJet Airlines Inc.          8.80\n10 EWR    BDL   EV      ExpressJet Airlines Inc.          6.78\n# ℹ 429 more rows\n\n\n\n\nd.\n\nbest_worst_airlines = flights_c |&gt;\n  group_by(origin, dest) |&gt;\n  summarise(\n    best_airline = name[which.min(avg_arr_delay)],\n    best_delay = min(avg_arr_delay),\n    worst_airline = name[which.max(avg_arr_delay)],\n    worst_delay = max(avg_arr_delay),\n    .groups = \"drop\"\n  )\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nbest_worst_airlines\n\n# A tibble: 223 × 6\n   origin dest  best_airline             best_delay worst_airline    worst_delay\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1 EWR    ALB   ExpressJet Airlines Inc.      14.4  ExpressJet Airl…       14.4 \n 2 EWR    ANC   United Air Lines Inc.         -2.5  United Air Line…       -2.5 \n 3 EWR    ATL   Endeavor Air Inc.             -6.25 ExpressJet Airl…       19.5 \n 4 EWR    AUS   Southwest Airlines Co.       -11.2  United Air Line…        4.28\n 5 EWR    AVL   ExpressJet Airlines Inc.       8.80 ExpressJet Airl…        8.80\n 6 EWR    BDL   ExpressJet Airlines Inc.       6.78 United Air Line…       22.6 \n 7 EWR    BNA   Southwest Airlines Co.        -2.13 ExpressJet Airl…       17.7 \n 8 EWR    BOS   ExpressJet Airlines Inc.      -4.01 JetBlue Airways         6.87\n 9 EWR    BQN   United Air Lines Inc.         10.9  United Air Line…       10.9 \n10 EWR    BTV   ExpressJet Airlines Inc.      12.2  ExpressJet Airl…       12.2 \n# ℹ 213 more rows\n\n\n\n\nd.\n\nbest_worst_airlines |&gt;\n  mutate(delay_diff = worst_delay - best_delay) |&gt;\n  arrange(desc(delay_diff)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 7\n  origin dest  best_airline      best_delay worst_airline worst_delay delay_diff\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n1 JFK    ATL   Endeavor Air Inc.       1.40 ExpressJet A…         128       127.\n\n\n\n\n\n10 Question 7\nHere are the names of the columns from the data:\n\n\n [1] \"id\"             \"age\"            \"gender\"         \"height\"        \n [5] \"weight\"         \"blood_type\"     \"disease_status\" \"cholesterol\"   \n [9] \"glucose\"        \"smoker\"         \"exercise\"       \"income\"        \n[13] \"education\"      \"region\"         \"marital_status\"\n\n\nUsing views_cols() to check the columns with missing entries, typos and any other inconsistencies.\n\n\n[1] \"id\"\n [1] \"id_1\"  \"id_7\"  \"id_29\" \"id_11\" \"id_36\" \"id_46\" \"id_26\" \"id_19\" \"id_18\"\n[10] \"id_39\" \"id_21\" \"id_20\" \"id_16\" \"id_17\" \"id_14\" \"id_43\" \"id_9\"  \"id_37\"\n[19] \"id_23\" \"id_47\"\n[1] \"30 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"age\"\n[1] 55 27 79 26 64\n[1] \"_____________________\"\n[1] \"gender\"\n[1] \"male\"   \"femal\"  \"female\"\n[1] \"_____________________\"\n[1] \"height\"\n[1] 199.2 174.9 183.6 180.4    NA\n[1] \"_____________________\"\n[1] \"weight\"\n[1] 72.2 87.0 83.4 79.1 99.3\n[1] \"_____________________\"\n[1] \"blood_type\"\n[1] \"O\"  \"A\"  \"B\"  \"AB\"\n[1] \"_____________________\"\n[1] \"disease_status\"\n[1] \"diseased\" \"Healthy\"  \"healthy\" \n[1] \"_____________________\"\n[1] \"cholesterol\"\n[1] 195 248 196 221 224\n[1] \"_____________________\"\n[1] \"glucose\"\n[1]  79  91  90 101  NA\n[1] \"_____________________\"\n[1] \"smoker\"\n[1] \"no\"  \"yes\"\n[1] \"_____________________\"\n[1] \"exercise\"\n[1] \"occasional\" \"none\"       \"regular\"   \n[1] \"_____________________\"\n[1] \"income\"\n[1] 96351 37940 68866 21516 35418\n[1] \"_____________________\"\n[1] \"education\"\n[1] \"master\"     \"bachelor\"   \"highschool\" \"PhD\"       \n[1] \"_____________________\"\n[1] \"region\"\n[1] \"West\"  \"South\" \"East\"  \"North\"\n[1] \"_____________________\"\n[1] \"marital_status\"\n[1] \"single\"   \"married\"  \"widowed\"  \"divorced\"\n[1] \"_____________________\"\n\n\nWarning: Not all unique entries displayed for these non-numeric cols: id\n\n\nFrom the results above\nThe following are the columns with missing entries:\n\nheight\nglucose\n\nThe following are the columns with typos:\n\ngender\ndisease_status",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Question 1</span>"
    ]
  }
]